{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948fb931-67bc-4be4-9267-db3ebf56ef56",
   "metadata": {},
   "source": [
    "## Code to data clean step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf0150-2c6c-4704-9340-1d72a184aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import os\n",
    "\n",
    "# # Define file paths\n",
    "# file_path = r\"C:\\Users\\wenjie.soh.2022\\Desktop\\TextMining\\project\\emails.csv\"\n",
    "# output_file = r\"C:\\Users\\wenjie.soh.2022\\Desktop\\TextMining\\project\\email_cleaned.csv\"\n",
    "# invalid_rows_log = r\"C:\\Users\\wenjie.soh.2022\\Desktop\\TextMining\\project\\idk.csv\"\n",
    "\n",
    "# # Define batch size \n",
    "# batch_size = 5000\n",
    "\n",
    "# # Function to validate email format\n",
    "# def is_valid_email_format(email):\n",
    "#     \"\"\"Check if the email contains necessary fields.\"\"\"\n",
    "#     required_fields = [\"Message-ID:\", \"From:\", \"To:\", \"Date:\", \"Subject:\"]\n",
    "#     return all(field in str(email) for field in required_fields)\n",
    "\n",
    "# # Function to extract email components\n",
    "# def extract_email_components(email, row_index):\n",
    "#     \"\"\"Extract metadata and content from an email.\"\"\"\n",
    "#     components = {\n",
    "#         \"Message-ID\": None, \"Date\": None, \"Time\": None, \"From\": None, \"To\": None, \"Subject\": None,\n",
    "#         \"Mime-Version\": None, \"Content-Type\": None, \"Content-Transfer-Encoding\": None,\n",
    "#         \"X-From\": None, \"X-To\": None, \"X-cc\": None, \"X-bcc\": None, \"X-Folder\": None,\n",
    "#         \"X-Origin\": None, \"X-FileName\": None, \"Content\": None\n",
    "#     }\n",
    "\n",
    "#     # Define regex patterns\n",
    "#     patterns = {\n",
    "#         \"Message-ID\": r\"Message-ID:\\s*(<.*?>)\", \"Date\": r\"Date:\\s*(.+)\", \"From\": r\"From:\\s*(.+)\",\n",
    "#         \"To\": r\"To:\\s*(.+)\", \"Subject\": r\"Subject:\\s*(.*)\", \"Mime-Version\": r\"Mime-Version:\\s*(.+)\",\n",
    "#         \"Content-Type\": r\"Content-Type:\\s*(.+)\", \"Content-Transfer-Encoding\": r\"Content-Transfer-Encoding:\\s*(.+)\",\n",
    "#         \"X-From\": r\"X-From:\\s*(.+)\", \"X-To\": r\"X-To:\\s*(.+)\", \"X-cc\": r\"X-cc:\\s*(.*)\",\n",
    "#         \"X-bcc\": r\"X-bcc:\\s*(.*)\", \"X-Folder\": r\"X-Folder:\\s*(.+)\", \"X-Origin\": r\"X-Origin:\\s*(.+)\",\n",
    "#         \"X-FileName\": r\"X-FileName:\\s*(.+)\"\n",
    "#     }\n",
    "\n",
    "#     for key, pattern in patterns.items():\n",
    "#         match = re.search(pattern, email, re.MULTILINE)\n",
    "#         if match:\n",
    "#             components[key] = match.group(1).strip()\n",
    "\n",
    "#     if components[\"Date\"]:\n",
    "#         date_parts = components[\"Date\"].split(\" \")\n",
    "#         if len(date_parts) > 3:\n",
    "#             components[\"Date\"], components[\"Time\"] = \" \".join(date_parts[:3]), date_parts[3]\n",
    "\n",
    "#     content_match = re.split(r\"X-FileName:.*?\\n\", email, flags=re.S)\n",
    "#     components[\"Content\"] = content_match[1].strip() if len(content_match) > 1 else None\n",
    "\n",
    "#     print(f\"Processing row {row_index} âœ…\")\n",
    "#     return components\n",
    "\n",
    "# # Ensure the output file is not corrupted\n",
    "# if os.path.exists(output_file):\n",
    "#     try:\n",
    "#         pd.read_csv(output_file)  # Try opening it\n",
    "#     except Exception:\n",
    "#         print(\"Corrupted file detected. Deleting and creating a new one.\")\n",
    "#         os.remove(output_file)\n",
    "\n",
    "# # Load the CSV file\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Prepare to store valid and invalid data\n",
    "# valid_data = []\n",
    "# invalid_data = []\n",
    "# headers = [\"file\", \"message\"]\n",
    "\n",
    "# # Process the dataframe row by row (skipping header implicitly via iteration)\n",
    "# for index, row in df.iterrows():\n",
    "#     file_name = row.get(\"file\", \"\")\n",
    "#     message = row.get(\"message\", \"\")\n",
    "\n",
    "#     # Remove leading/trailing spaces\n",
    "#     message = str(message).strip() if message else \"\"\n",
    "\n",
    "#     # Skip empty messages\n",
    "#     if not message:\n",
    "#         continue\n",
    "\n",
    "#     # Separate valid and invalid rows\n",
    "#     if is_valid_email_format(message):\n",
    "#         valid_data.append([file_name, message])\n",
    "#     else:\n",
    "#         invalid_data.append([file_name, message])\n",
    "\n",
    "#     # Process batch when reaching batch_size\n",
    "#     if len(valid_data) >= batch_size:\n",
    "#         print(f\"\\nðŸ”¹ Processing batch of {batch_size} valid emails...\")\n",
    "\n",
    "#         df_valid = pd.DataFrame(valid_data, columns=headers)\n",
    "#         parsed_data = df_valid[\"message\"].apply(lambda x, idx: extract_email_components(x, idx), args=(df_valid.index,)).apply(pd.Series)\n",
    "#         df_cleaned = pd.concat([pd.Series(range(1, len(df_valid) + 1), name=\"No.\"), df_valid[\"file\"], parsed_data], axis=1)\n",
    "\n",
    "#         # Determine mode based on file existence\n",
    "#         mode = \"a\" if os.path.exists(output_file) else \"w\"\n",
    "        \n",
    "#         # Append to output CSV file\n",
    "#         df_cleaned.to_csv(output_file, mode=mode, header=(mode == \"w\"), index=False)\n",
    "\n",
    "#         # Clear memory\n",
    "#         valid_data = []\n",
    "\n",
    "# # Save any remaining valid rows\n",
    "# if valid_data:\n",
    "#     df_valid = pd.DataFrame(valid_data, columns=headers)\n",
    "#     parsed_data = df_valid[\"message\"].apply(lambda x: extract_email_components(x, df_valid[df_valid[\"message\"] == x].index[0])).apply(pd.Series)\n",
    "\n",
    "#     df_cleaned = pd.concat([pd.Series(range(1, len(df_valid) + 1), name=\"No.\"), df_valid[\"file\"], parsed_data], axis=1)\n",
    "\n",
    "#     mode = \"a\" if os.path.exists(output_file) else \"w\"\n",
    "    \n",
    "#     df_cleaned.to_csv(output_file, mode=mode, header=(mode == \"w\"), index=False)\n",
    "\n",
    "# # Save invalid rows\n",
    "# if invalid_data:\n",
    "#     df_invalid = pd.DataFrame(invalid_data, columns=headers)\n",
    "#     df_invalid.to_csv(invalid_rows_log, mode=\"w\", index=False)\n",
    "\n",
    "# # Ensure at least one valid output exists\n",
    "# if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:\n",
    "#     pd.DataFrame({\"No Valid Data\": [\"No valid emails found\"]}).to_csv(output_file, index=False)\n",
    "\n",
    "# # print(f\"Processing complete! Cleaned dataset saved to: {output_file}\")\n",
    "# # print(f\"Invalid rows logged in: {invalid_rows_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4ecce-22a1-42f0-9b74-b6dd90d46487",
   "metadata": {},
   "source": [
    "## Next is to apply the roles to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a99ba-6e10-4f0c-b6d3-37aff04ba589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "438781c1-36f9-48e4-bed5-23480e10b477",
   "metadata": {},
   "source": [
    "## Next is to trim the dataset columns and apply word count on the content column and further filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1596b-b157-4348-a4c1-98fa15458d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)   # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Adjust the width to display full table\n",
    "pd.set_option('display.max_colwidth', None)  # Adjust column width to avoid truncation\n",
    "\n",
    "\n",
    "# Check file path\n",
    "enron_emails_df=pd.read_csv('email_cleaned_with_roles.csv')\n",
    "\n",
    "trimmed_enron_df=enron_emails_df.drop(columns=[\"X-FileName\",\"X-Origin\",\"Content-Transfer-Encoding\",\"Content-Type\",\"X-Folder\",\"Mime-Version\",\"X-To\",\"X-From\",'file','No.'])\n",
    "\n",
    "\n",
    "filtered_df = trimmed_enron_df[~trimmed_enron_df[\"From\"].isin([\"enron.announcements@enron.com\", \"issuealert@scientech.com\"])]\n",
    "\n",
    "filtered_df.to_csv(\"trimmed_enron_emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757625af-b6ea-426e-b3bd-da69ff765498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('trimmed_enron_emails.csv')\n",
    "\n",
    "# test.to_csv('test.csv', index=False)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ed1a7-d0f4-4ac6-9554-96d0b91504d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_sentence_word_counts(text):\n",
    "    # Split into sentences using . ! ? (followed by space or end of string)\n",
    "    sentences = re.split('[.!?]+(?:\\s+|$)', text)\n",
    "    # Remove empty sentences and clean each one\n",
    "    word_counts = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Skip empty strings\n",
    "            # Remove punctuation and newlines\n",
    "            clean_sentence = sentence.translate(str.maketrans('', '', string.punctuation + '\\n'))\n",
    "            # Split into words and count\n",
    "            words = clean_sentence.split()\n",
    "            if words:  # Only include if there are words\n",
    "                word_counts.append(len(words))\n",
    "    return word_counts\n",
    "tqdm.pandas()\n",
    "# # Apply the function to the 'Content' column\n",
    "df['Sentence_Word_Counts'] = df['Content'].progress_apply(get_sentence_word_counts)\n",
    "\n",
    "def sum_word_counts(word_count_list):\n",
    "    return sum(word_count_list)\n",
    "\n",
    "\n",
    "# Apply the function to the 'Sentence_Word_Counts' column\n",
    "df['Total_Sentence_Word_Count'] = df['Sentence_Word_Counts'].progress_apply(sum_word_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71538d1a-cff0-4ac8-8672-03fbdbfc07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['Total_Sentence_Word_Count'] >= 10]\n",
    "df_filtered = df_filtered.drop('Sentence_Word_Counts', axis=1)\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "df_filtered.to_csv('main_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5051d-9771-4db6-a610-142dae6017a1",
   "metadata": {},
   "source": [
    "## Data cleaning ends here, next is further text pre-processing and maybe abit more data cleaning on the rows/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bd2f3-d945-47ac-828e-017c8e63946e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6962a-480a-4099-9eec-69426aaa9b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
