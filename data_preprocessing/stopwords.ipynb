{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing: Custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import stanza\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 480062 entries, 0 to 480061\n",
      "Data columns (total 11 columns):\n",
      " #   Column                     Non-Null Count   Dtype \n",
      "---  ------                     --------------   ----- \n",
      " 0   Message-ID                 480062 non-null  object\n",
      " 1   Date                       480062 non-null  object\n",
      " 2   Time                       480062 non-null  int64 \n",
      " 3   From                       480062 non-null  object\n",
      " 4   To                         480062 non-null  object\n",
      " 5   Subject                    480062 non-null  object\n",
      " 6   X-cc                       480062 non-null  object\n",
      " 7   X-bcc                      480062 non-null  object\n",
      " 8   Content                    480062 non-null  object\n",
      " 9   Job_Title                  480062 non-null  object\n",
      " 10  Total_Sentence_Word_Count  480062 non-null  int64 \n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 40.3+ MB\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "enron_emails_df=pd.read_csv('../main_data.csv')\n",
    "enron_emails_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords (only the first time)\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "print(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize emails into words\n",
    "emails2 = [nltk.word_tokenize(email.lower()) for email in enron_emails_df['Content']]\n",
    "\n",
    "# Keep only alphabetic words\n",
    "emails3 = [[w for w in email if re.search('^[a-z]+$', w)] for email in emails2]\n",
    "\n",
    "# Remove stopwords\n",
    "emails4 = [[w for w in doc if w not in stop_list] for doc in emails3]\n",
    "\n",
    "# Flatten the list of lists\n",
    "all_words = [word for email in emails4 for word in email]\n",
    "\n",
    "# Count word frequencies\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# Get the 50 most common words\n",
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Custom Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ect', 6045), ('pm', 3949), ('mark', 3276), ('new', 2993), ('cc', 2936), ('may', 2814), ('iso', 2759), ('ferc', 2751), ('said', 2533), ('time', 2340), ('e', 2285), ('mary', 2176), ('also', 2119), ('call', 1941), ('list', 1601), ('need', 1550), ('one', 1509), ('know', 1504), ('ees', 1504), ('get', 1446), ('data', 1366), ('rate', 1349), ('like', 1303), ('us', 1299), ('make', 1212), ('see', 1204), ('sent', 1198), ('http', 1192), ('send', 1186), ('last', 1149)]\n"
     ]
    }
   ],
   "source": [
    "# Check the frequency of words with length <= 4\n",
    "\n",
    "text2_long_words = [w for w in all_words if len(w) <= 4]\n",
    "fdist2 = FreqDist(text2_long_words)\n",
    "print(fdist2.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192\n"
     ]
    }
   ],
   "source": [
    "# Check the frequency of specific words\n",
    "\n",
    "# print(all_words.count('mail')) #779\n",
    "# print(all_words.count('forwarded')) #1577\n",
    "# print(all_words.count('thank')) #666\n",
    "print(all_words.count('http')) #1192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('corp', 'time'), ('time', 'fri'), ('first', 'time'), ('time', 'sat'), ('real', 'time'), ('time', 'date'), ('describe', 'time'), ('next', 'time'), ('houston', 'time'), ('people', 'time')]\n"
     ]
    }
   ],
   "source": [
    "# Assess if a specific word is useful based on collocation pairs \n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "# Step 1: Create a BigramCollocationFinder from the tokenized word list\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_words)\n",
    "\n",
    "# Step 2: Get all bigrams containing \"time\"\n",
    "bigrams_with_said = [bigram for bigram in bigram_finder.ngram_fd.keys() if 'time' in bigram]  # edit to check diff collocation pairs\n",
    "\n",
    "# Step 3: Sort bigrams by frequency (most common first)\n",
    "sorted_bigrams = sorted(bigrams_with_said, key=lambda bigram: bigram_finder.ngram_fd[bigram], reverse=True)\n",
    "\n",
    "# Step 4: Print the top 10 most common bigrams with \"time\"\n",
    "print(sorted_bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Add more custom stopwords\n",
    "custom_stopwords = {\"enron\", \"email\", \"subject\", \"company\", \"corporate\", \"mary\", \"hain\", \"hou\", 'ect', \"mark\", \"hainhouect\", \"haedickehouectect\", \"ect\", \"please\", \"would\", \"pm\", \"cc\", \"may\", \"e\", \"forwarded\", \"attached\", \"attach\", \"thanks\", \"could\", \"mail\", \"mailing\", \"bcc\", \"dear\", \"thru\", \"forwarded\", \"hi\", \"hello\", \"much\", \"really\", \"susan\", \"j\", \"q\", \"p\", \"pls\", \"thank\", \"ps\", \"sorry\", \"also\", \"might\", \"must\", \"call\", \"fw\", \"fwd\", \"date\", \"sincerely\", \"sent\", \"http\", \"list\", \"asap\", \"corp\"}  # Additional custom stopwords\n",
    "stop_words.update(custom_stopwords)  # Add to NLTK stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
